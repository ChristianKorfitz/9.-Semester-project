\subsection{Classification algorithm}

The most important function of the classifier is to correctly separate the noise components from the signals of interest. However, it is highly probable that components do not purely contain either noise or signal, and that the characteristics of the various types of artefacts overlap. Thus, in a the context of a classification model, noise and signal are not left as two well-defined clusters in both the temporal and spatial domain, and the decision boundaries separating noise from signal would be complex. \cite{Salimi-Khorshidi2014} \\
A first step in simplifying the decision making process was to divide features into subsets, partially through a feature selection process. The reasoning for this was that some components might have more clear signal fluctuations shown only in either temporal or spatial features or other feature subsets. Let $S = S_t{\cup}S_s$ denote the full feature set, where $S_t$ and $S_s$ are the full temporal and spatial feature sets, respectively. On these subsets FIX applied feature selection based on F-scores for each feature to calculate signal-noise discrimination and logistic regression and a linear support vector machine (SVM) for ranking features. Then $S_{sel{\supset}S}$ denotes a feature subset of $S$, which contains both temporal and spatial features, denoted $S_{t-sel}$ and $S_{s-sel}$ respectively. The resulting subsets were then $S, S_t, S_s, S_{sel}, S_{t-sel}$ and $S_{s-sel}$, which were used to train the classifier (all the subsets had a column vector containing the label of each component). \\
To achieve a robust classification of components using these subsets of extracted features, there exist no ultimately ideal classifier. Each classifier has its weaknesses and strengths. The k-nearest neighbor algorithm (kNN) is a decent local classifier, but struggles with detecting patterns in a full data set; SVMs are great at finding decision boundaries with maximum between cluster variance; decision trees are great at finding complex decision boundaries that can be described by a set of if-then rules. To compensate for a classifier’s weakness through another’s strength, FIX employed an ensemble learning method called classifier stacking. Here the output of several single “lower level” classifiers became the input in a “higher-level” classifier - a fusion of different classifiers. The “lower level” classifiers consisted of a decision tree, kNN, SVM with radial basis function $(RBF)$ kernel $(SVM_r)$, SVM with polynomial kernel $(SVM_p)$ and linear SVM $(SVM_l)$. Another reasoning for using these specific classifiers was that they could produce a probability output between 0 and 1, where 0 denoted perfect noise and 1 denoted perfect signal. Each of the six subsets were fed to each of the 5 classifiers, thus producing 30 probability values (5 x 6 matrix) between 0 and 1 which functioned as input to the “higher level” classifier. The “higher level” classifier consisted of multiple classifiers: $SVM_l$, $SVM_r$, random forest, and conditional-inference tree. \\
Thus, training this ensemble learner algorithm consisted of selecting the data subsets, training the “lower-level” level classifiers, whose output was the input of the “higher level” classifier. The output of the "higher level" classifiers was the probability of a component being noise or signal; labeling a components as either signal, unknown or unclassified noise. A graphical illustration of the classifier design can be seen in \figref{fig:classifier}. %\fxnote{(elaborate on the structure of the higher level classifier.)}

\begin{figure}[H]                 
	\includegraphics[width=.95\textwidth]{figures/bMethods/classifier}  
	\caption{An illustration of the classifier design. The six feature subsets can be seen fed to five different classifiers in the lower level classifier. The total output from the 30 classifiers was a 5 $\times$ 6 probability matrix, containing the probability estimated by each classifier. This matrix was then fed to the higher level classifier consisting of four classifiers, deciding the final component label. Illustration adapted from \cite{Salimi-Khorshidi2014}.}
	\label{fig:classifier} 
\end{figure}

\subsection{Estimating component label threshold}
The classifier required setting a threshold on how conservative the classifier should be at including potentially noisy signal components. This threshold scale varied from 1-100, where 1 was the most soft, meaning that almost no components containing some portion of signal of interest were labeled as noise. \\
To be able to perform a quantitative evaluation of which threshold to choose, the FIX tool provided the leave-one-out(LOO) analysis option. On the data set containing \textit{n} outputs (e.g. one for each heat run per participant), a cross-validation was performed, where each fold used a \textit{n-1} data set for training and the obtained decision boundary was tested on the data set left out. This LOO was carried out using the following thresholds: 1, 2, 5, 10, 20, 30, 40, 50. For each heat run and each threshold the following was additionally calculated: true-positive ratio (TPR), true-negative ratio (TNR) and a weighted ratio (WR), $\frac{3\cdot TPR+TNR}{4}$, which favored the TPR. Summarizing these values as the mean across all heat runs yielded an accuracy matrix, from which a decision on which threshold to choose could be based upon. \\
After performing the LOO classification with the training data from this project, it was observed that a TPR of NaN for all thresholds was calculated for two of the heat runs due to the heat runs not containing any signal components. This resulted in the mean TPR and the WR being NaN for all thresholds. These two heat runs were therefore excluded from the project. The resultant accuracy matrix of the LOO classification can be seen in \tabref{tab:decisionmatrix}.

\begin{table}[H] 
	\caption{Accuracy matix from the LOO classifier. TPR denotes the mean percentage of true signals correctly classified. TNR denotes the mean percentage of true noise correctly classified. WR is a weighted ratio that favors TPR. The more the threshold increases the lower the TPR and the higher the TNR.}\label{tab:decisionmatrix}
	\begin{tabular}{l|llllllll}
		Threshold:                              & 1    & 2    & 5    & 10   & 20   & 30   & 40   & 50   \\ \hline
		TPR:                                    & 99.1 & 98.6 & 98.4 & 97.3 & 96.3 & 94.3 & 92.4 & 91.4 \\ %\cline{1-1}
		TNR:                                    & 78.1 & 80.9 & 85.5 & 90.2 & 93.7 & 95.5 & 96.6 & 97.7 \\ %\cline{1-1}
		WR: & 93.9 & 94.2 & 95.2 & 95.5 & 95.7 & 94.6 & 93.4 & 93.0
	\end{tabular}
\end{table}


For this project it was important that as much as possible signal was kept, while not including too much noise. This demand was well summarized by the WR. Thus, the threshold with the highest WR (20) was initially chosen. However, it was recommended by the FIX developers to validate the most preferred thresholds qualitatively, before making a final decision. The test data set was therefore run on the trained classifier using the thresholds with a WR above 95 \% (5, 10 and 20). Checking the classified labels on three participant per threshold, left the initial decision on choosing a threshold of 20 to stand.

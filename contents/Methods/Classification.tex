\subsection{Classification algorithm}

The most important function of the classifier is to correctly separate the noise components from the signals of interest. However, it is highly probable that components do not purely contain either noise or signal, and that the characteristics of the various types of artefacts overlap. Thus, in a the context of a classification model, noise and signal are not left as two well-defined clusters in both the temporal and spatial domain, and the decision boundaries separating noise from signal will be complex. \cite{Salimi-Khorshidi2014} \\
A first step in simplifying the decision making process was to divide features into subsets, partially through a feature selection process. The reasoning for this was that some components might have more clear signal fluctuations shown only in either temporal or spatial features or other feature subsets. Let $S = S_t{\cup}S_s$ denote the full feature set, where $S_t$ and $S_s$ are the full temporal and spatial feature sets. On these subsets FIX applies feature selection based on F-scores for each feature to calculate signal-noise discrimination and logistic regression and a linear support vector machine (SVM) for ranking features. Then $S_{sel{\supset}S}$ denotes a feature subset of $S$, which contains both temporal and spatial features, denoted $S_{t-sel}$ and $S_{s-sel}$ respectively. The resulting subsets are then $S, S_t, S_s, S_{sel}, S_{t-sel}$ and $S_{s-sel}$, which were used to train the classifier (all the subsets have a column vector containing the label of each component). \\
To achieve a robust classification of components using these subsets of extracted features, there exist no ultimately best classifier. Each classifier has its weaknesses and strengths. The k-nearest neighbor algorithm (kNN) is a decent local classifier, but struggles with detecting patterns in a full dataset; SVMs is great at finding decision boundaries with maximum between cluster variance; decision trees are great at finding complex decision boundaries that can be described by a set of if-then rules. To compensate a classifier’s weakness through another’s strength, FIX employed an ensemble learning method called classifier stacking. Here the output of several single “lower level” classifiers becomes the input in a “higher-level” classifier - a fusion of different classifiers. The “lower level” classifiers consists of decision tree, kNN, SVM with radial basis function $(RBF)$ kernel $(SVM_r)$, SVM with polynomial kernel $(SVM_p)$ and linear SVM $(SVM_l)$. Another reasoning for using these specific classifiers was that they could produce a probability output between 0 and 1, where 0 denoted perfect noise and 1 denoted perfect signal. Each of the six subsets were fed to each of the 5 classifiers, thus producing 30 probability values (5 x 6 matrix) between 0 and 1 which function as input to the “higher level” classifier. The “higher level” classifier consists of multiple classifiers: $SVM_l$, $SVM_r$, random forest, and conditional-inference tree, that learns how to best combine the inputs. \\
Thus, training this ensemble learner algorithm consisted of selecting the data subsets, training the “lower-level” level classifiers, whose output was the input of the “higher level” classifier, whose output was the probability of a component being noise or signal.\fxnote{(Insert illustration/flowchart of the ensemble learner and elaborate on the structure of the higher level classifier.)}

\subsection{MELODIC}

After the implementation of the standard preprocessing the subsequent step of the FIX implementation was to apply an ICA to the functional scan. ICA has for many years been a favored exploratory analysis tool in fMRI, as it allows for a decomposition of the signal information into estimated sources. Hence, it would be possible to differentiate between sources containing signal of interest, physiologic noise and other artifact contributers. ICA employs blind source separation techniques expressed as having a random set of variables (the acquired observations), as a linear combinations of statistically independent component variables (the sources). \cite{Beckmann2004} \\
To achieve a decomposition of the BOLD signal into components each consisting of an independent source signal MELODIC was utilized. The following section will document how the ICA was performed on fMRI data through the use of MELODIC. An introduction of general ICA can be found in \secref{sec:ICA}. \\
MELODIC is similar to general ICA, but includes a probabilistic approach, called a Probabilistic ICA (PICA). It still expresses the assumption that a vector of $p$ observations is generated from a set of $q$ statistically independent non-Gaussian sources via a linear instantaneous mixing process. In addition, the probabilistic approach assumes that this process is corrupted by Gaussian noise $n(t)$ \cite{Beckmann2004}. This can be expressed as, 

\begin{equation}
x_i = As_i + \mu + n_i  \hspace{3em} \forall i \in V.
\end{equation}     

where $x_i$ denotes a $p$-dimensional column vector of each measurement in a specific voxel $i$, which result in a column vector for each voxel $V$. $s_i$ denotes a $q$-dimensional column vector of the independent source signal contained in the data. $\mu$ denotes the mean of the observations $x_i$. $A$ is the $p\times q$ mixing matrix and $n_i$ is the Gaussian noise, which is estimated using a probabilistic principal component analysis (PPCA). The input observations for each entire functional scan was set up as,
\begin{equation}
 \begin{bmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,217600} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,217600} \\
\cdots & \cdots & \cdots & \vdots \\
x_{193,1} & x_{193,2} & \cdots & x_{193,217600} 
\end{bmatrix}  
\end{equation}

For each of the 217600 voxels 193 measurements were made, resulting in a 193 $\times$ 217600 input matrix. In order to estimate the independent sources, the solution lies in finding a linear mixing matrix $W$:
\begin{equation}
\widehat{s}= Wx
\end{equation}  
such that $\widehat{s}$ is an optimal approximation of independent source signals. 
\subsubsection{PPCA}

Before finding the mixing matrix PPCA is employed to estimate the "true" number of sources in the signal. A general PCA is an often used method of reducing the dimensionality of a data set. A general introduction to PCA can be found in \secref{sec:PCA}. The PPCA is used to find a linear subspace of sources. PPCA incorporates a probabilistic model, which is used to asses the number of true sources and random noise observed. Before employing the PPCA the input data was variance-normalized. Finding the subspace of sources was done by using Singular Value Decomposition (SVD), which left the subspace with a set of sources which were orthogonal to each other and thereby being uncorrelated. Through a probabilistic Bayesian framework the number of "true" sources was estimated from the covariance matrix incorporating the information of unstructured noise that was found in the weakest eigenvectors of the PCA. Hence, through the PPCA a linear subspace of "true" sources was estimated. Furthermore, the PPCA estimated subspace was spatially whitened, which is beneficial as it simplifies the later ICA estimation by lowering the amount of parameters needed to be estimated.  \\
In this project there was not made use of the dimensionality estimation for reasons explained later in \secref{sec:optimal}. Hence, only a fixed number of sources were estimated being the first 25 "true" components describing the most of the variance.

\subsubsection{PICA}

The next step was to solve for the linear subspace, by estimating the mixing matrix, in order to achieve independent source signals. There are multiple approaches in which this can be achieved. The main idea of the PICA is to try and achieve non-gaussian source distributions by incorporating and accounting for the Gaussian noise, which is present in the BOLD signal. Hence, estimating the mixing matrix $W$ was based on maximizing non-Gaussianity. Non-Gaussianity was maximized by approximating neg-entropy. \\
The result of estimating the mixing matrix was a $193 \times 25$ matrix containing 25 independent components with a time course for each. In order to get a spatial map visualizing where the activation of a specific source was present, each voxel's time course was projected onto the columns of the unmixing matrix. These maps were then transformed into Z-score maps using the standard deviation of the prior estimated noise, which enabled the differentiation between source signal and background noise. Finally, a single spatial IC map for each component was made by fitting a Gaussian Mixture model to the individual IC maps to find voxel locations that were significantly activated through the IC time course. This made a probability map for each component, which was then thresholded to only capture meaningful activation. The resulting output of employing the MELODIC ICA was a $193 \times 25$ matrix of temporal components and a $25 \times 217600$ matrix of spatial components. A summarization of the input and output of MELODIC is illustrated in \eqref{eq:ica}, where the left-hand side was the input and the right-hand side was the two output matrices. 
 
\begin{equation} \label{eq:ica}
 \begin{bmatrix}
	x_{1,1}  & \cdots & x_{1,217600} \\
	x_{2,1}  & \cdots & x_{2,217600} \\
	\cdots   & \cdots & \vdots \\
	x_{193,1}& \cdots & x_{193,217600} 
\end{bmatrix}
= 
\begin{bmatrix}
	x_{1,1}  & \cdots & x_{1,25} \\
	x_{2,1}  & \cdots & x_{2,25} \\
	\vdots   & \cdots & \vdots \\
	x_{193,1} & \cdots & x_{193,25} 
\end{bmatrix}
\bigotimes 
\begin{bmatrix}
	x_{1,1} & x_{1,2} & \cdots & x_{1,217600} \\
	\cdots & \cdots & \cdots & \vdots \\
	x_{25,1} & x_{25,2} & \cdots & x_{25,217600} 
\end{bmatrix}
\end{equation}
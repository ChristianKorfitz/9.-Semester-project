\chapter{Discussion}
\subsection*{Performance of FSL FIX}
The use of 3.0 tesla scanners for fMRI improves brain activation detection, but simultaneously captures more physiological noise. The amount of noise might increase when scanning brain activity through a noxious stimuli study design. FSL FIX is a newly developed denoising algorithm that can be used to identify various noise components and denoise a large amount of data. It utilizes an ICA approach in identifying sources of signal in the data, which are used to train a classifier. The trained classifier is used to identify source signals on a large test set, where the noise components identified subsequently can be removed from the data. The primary aim for this project was therefore to evaluate FSL FIX in comparison to standard preprocessing on a large data set acquired from a noxious stimuli protocol, to assess if FSL FIX could generate a higher signal-to-noise ratio. \\
Using the FSL FIX preprocessing method shows promising results when compared to standard preprocessing. The FSL FIX preprocessed data showed higher activation in the insular cortex, anterior cingulate cortex and basal ganglia compared to the standard preprocessed data. These are all acknowledged activation sites in relation to pain perception. Furthermore, activation in the standard preprocessed data was higher in white matter and the default mode network, when compared to the FSL FIX preprocessed data. These are areas of no interest in pain related brain activation. This indicates that FSL FIX attenuates activation in areas of no interest, when compared to data preprocessed with standard preprocessing pipeline. These results leaves FSL FIX as a favored preprocessing method for denoising large amounts of data acquired in a noxious stimuli study design compared to standard preprocessing. 

A factor that might have had a significant influence on the effect of the FIX preprocessing was the number of components chosen to be estimated for this project. The MELODIC algorithm provided a default setting of automatically estimating how many sources the data contained. The FSL developers recommended using this automatic estimation for estimation the optimal number of sources the data was comprised of. However, for this project it was chosen to estimate a fixed number of 25 components for every heat run. This was based on a preliminary qualitative assessment, where 15, 20, 25, 30 and the automatically estimated set of components were extracted respectively for the same randomly selected heat runs. Assessing the ratio between components only containing noise or signal of interest and components containing a mix of signal of interest and noise, it was decided that estimating 25 components per heat run would yield the best ratio. However, it was not assessed whether 25 components were enough to contain pure signal of interest components in every heat run. As many heat runs were found to be highly noisy, and as the MELODIC algorithm ranked the components based on the variance explained by each component, 25 components might not have been enough to estimate all signal of interest encapsulated in the data. Using 25 components per heat run resulted in some heat runs to not contain any signal of interest components and some to only contain 1 signal of interest component. Thus, limiting the number of components might have filtered some signal of interest out. Either through underestimating the “true” number of sources, and thereby not estimating all signal of interest sources, or due to signal being disguised in noise and thus labeled as noise. In future studies utilizing MELODIC for ICA before employing FSL FIX, it might be more beneficial to either use the automatic estimation of components as it in general estimated >40 components during the qualitative assessment, or fix the number of components to >25. A downside of this approach, however, might be that some components could be fragmented, as the number of components extracted might be higher than the actual number of sources in the data. It is therefore recommended to assess the automatic estimation, which MELODIC produces, as the estimation of "true" sources is highly dependent on the amount and type of noise in each independent scan. \\

The hand labeling of components relied heavily on the investigator’s assessment, which was mainly based on information regarding the appearance of the various components presented in \cite{Salimi-Khorshidi2014, Griffanti2017}. This evaluation process involved some uncertainty, as the it was a resting state protocol used in the references and not a noxious stimuli protocol as used in this project. This uncertainty was especially occuring in the case of components comprised of several sources, and might have resulted in inconsistency in the labeling. Regarding this issue it might be beneficial to do a separate study that outlines more examples of the various types of components related to a noxious stimuli protocol. For such a study a protocol, where participants are informed to perform noise inducing actions at fixed time points, could be developed, e.g. taking deep breaths while receiving a noxious stimulus, performing minor/major movements while receiving a noxious stimulus etc. This might make the process of identifying the appropriate label easier and more accurate for future studies using hand labeling of independent components on data from a noxious stimuli protocol, and ultimately improve the signal-to-noise ratio even further. \\
When labeling data in future studies, it might be beneficial to create further labels to describe noise components, besides the ones provided by FSL. It was for instance hard to distinguish between physiological noise such as cardiac and respiratory. A common label for these two sources of noise might confuse the classifier less. Contrarily, the classifier only outputs components as either being signal of interest, unknown or unclassified noise. This means that as long as the labels used to train the classifier are correctly categorized as either one of those three labels, it might not have an impact. In that relation, it might be interesting to evaluate the classifier performance when only labeling components as either signal of interest, unknown or unclassified noise. If the classifier accuracy yields as high as when categorizing different types of noise, the labeling process will most likely be less labour intensive.

In the Individual Differences Project from which the heat stimuli data used in this project originates, noxious auditory and cold stimuli data was also acquired. To test the generalizability of the trained classifier on other noxious task-related data, the audio and cold data could be used as test data, respectively. If the classification on the new test data sets proved to be as accurate, the trained classifier would be a useful classifier for other noxious task-related data, and thus saving the scientific community time, when denoising large population data sets. \\

Comparing the performance indices of the LOO accuracy test of this project to the LOO results obtained by Salimi-Khorshidi et al. \cite{Salimi-Khorshidi2014} the TPR of this project was 96.3 \%, the TNR was 93.7 \%, leaving a  WR of 95.7 \%, where they archived a TPR of 99.6 \%, a TNR of 98.9 \% and a WR of 99.4 \%. The LOO test was performed on 99 runs of 6.5 minutes length from a task-related protocol for this project, where the LOO test was performed on 100 runs of 15 minutes length from a high quality resting-state protocol in the Salimi-Khorshidi et al. study. The number of runs the LOO tests was performed on are similar, which makes a more valid foundation for assessing the differences between the two tests. 
%(As the motivation for choosing a classification threshold was mostly based on the WR in this project and as it incorporates both the TPR and TNR, this performance indice will be the main focus of the comparison.) 
The WR was 3.7 percentage points higher in the Salimi-Khorshidi et al. study than this project, and had an almost perfect sensitivity of 99.6 \% - 3.3 percentage points higher than this project. A reason for the difference might be that Salimi-Khorshidi et al. used a resting-state protocol, compared to the task-related heat stimuli protocol used in this project. Delivering heat stimuli to participants might induce more noise from reactions of the stimuli, e.g. movement, hyper/hypoventilation, change in cardiac pulsation etc. This might have made components more difficult to identify, as the more sources could be mixed in each component. Another reason is that the amount of data in each run was more abundant in the study by Salimi-Khorshidi et al. Each run consisted of 15 minutes data of interest, where this study only focused on the 48$\degree$ stimuli parts of each run, leaving 90-105 seconds data of interest of this project. This might have made the components even more difficult do identify. However, a WR 95.7 \% still proved sufficiently high to denoise the test data more than only applying the standard preprocessing pipeline. 

\subsection*{Individual Differences}
The secondary aim of the project was to evaluate FSL FIX’s impact on the detection of individual differences compared to standard preprocessing, i.e. assessing the relation between participants’ subjective rating of pain intensity and the detected brain activity.
For the FSL FIX preprocessed data no activation was related to pain sensitivity when assessing this in a within group analysis. Similar lack of sensitivity related activation was found in the standard preprocessed data, besides a minor activation site localized between the brain hemispheres, which could be regarded as noise. Whether there is a relation between pain sensitivity and brain activation can therefore not be either confirmed or disproved following the results of this project. The result af not seeing any individual differences in brain activation contradicts what has been discovered earlier by Coghill et al. \cite{Coghill2003}. In this study, i.e. the anterior cingulate cortex were found to be related to the individuals' experience of pain. This adds to the ambiguity of the presence of individual differences.    

A potential reason for why there was not shown any relation between the reported VAS ratings and the brain activity might be due to some participants over/underplaying the pain intensity, compared to how much the brain regions associated with pain perception actually might have been activated. \\
Another reason could be that the analysis was carried out as a within group analysis using the mean of the 17 VAS ratings for each participant as input. There might be a large variability within a single subject, which would have been washed out by averaging the ratings. An alternative approach would have been to do the analysis on a within subject level. This approach would have accounted for the within subject variability in the VAS ratings, by including each individual rating in the analysis. 

